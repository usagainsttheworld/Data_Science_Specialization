#Which exercise did they do?
=============================================
The datasets are from a study with 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects. The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 

##Load Data

```{r}
setwd("~/Coursera/ML_predict-exerciseManner")
training_raw<-read.table("pml-training.csv", sep=",", na.string=c("NA",""), header=T)
testing_raw<-read.csv("pml-training.csv", sep=",", na.string=c("NA",""), header=T)
```

##Cleaning data

```{r}
#For the training data set, many variables contain lots of NAs (19216 NAs out of 19622 observations in the training data set). The number of observations for each class are 5580, 3797 ,3422 ,3216, and 3607 each. so it is unlikely that wether those variables are NAs or not is related to which class of exercise did they do. So all columns contains NAs are exluded from the datasets for the following analysis. Columns having unique values including "X", "user_name", "raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp", "new_window" ,"num_window" are not related to which class of exercise did they do, thus are removed from the datasets. Check for zero covariates, all variables are not zero covariates so no change should apply
library(ggplot2)
library(caret)
library(dplyr)
library(randomForest)
set.seed(123)
training_noNA<-training_raw[,sapply(training_raw, Negate(anyNA)), drop=F]
training<-training_noNA[,8:ncol(training_noNA)]
nzv<-nearZeroVar(training, saveMetrics=T)
dim(training)
#Cleaning data for testing_raw data set.Split the test set into validation set and test set half half, so there will be 50% training, 25% testing, and 25% validation
testing_noNA<-testing_raw[,sapply(testing_raw, Negate(anyNA)), drop=F]
testing_build<-testing_noNA[,8:ncol(testing_noNA)]
inbuild<-createDataPartition(y=testing_build$class, p=.5, list=F)
testing<-testing_build[inbuild,]
validition<-testing_build[-inbuild,]
dim(testing); dim(validition)
```

##Model building

```{r}
## First tried to perfored random forest model fitting on training data set, program running for hours. Then tried set train control method as "cv" with 3 folds instead of 10 folds (default). Excursion took about an hour with 500 trees generated with estimate error rate 0.01%. Next, I tried to reduce the number of trees to 50 and the excursion time went down to 10 mins with the error rate rise to 0.03%. Mac 1.86 GHz Intel Core 2 Duo was used for all analysis for this study. 
mod1<-train(training$class~., method="rf", data=training, ntree=50, trControl = trainControl(method="cv"), number=3)
mod1$finalModel
```

##Cross validation

```{r}
#Since the random forest model performs very well with 97% accuracy, the validation phase (for selecting approaches) was skiped for this study. Then, prediction was applied on validation data set to estimate the accuracy of the model. The prediction for the test set were correct in 100% of the cases.
prediction<-predict(mod1, newdata=testing)
confusionMatrix(prediction, testing$class)
```

##

